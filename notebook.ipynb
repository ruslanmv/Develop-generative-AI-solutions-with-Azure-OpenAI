{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "    \n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )\n",
    "    \n",
    "deployment_name='REPLACE_WITH_YOUR_DEPLOYMENT_NAME' #This will correspond to the custom name you chose for your deployment when you deployed a model. Use a gpt-35-turbo-instruct deployment. \n",
    "    \n",
    "# Send a completion call to generate an answer\n",
    "print('Sending a test completion job')\n",
    "start_phrase = 'Write a tagline for an ice cream shop. '\n",
    "response = client.completions.create(model=deployment_name, prompt=start_phrase, max_tokens=10)\n",
    "print(start_phrase+response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "load_dotenv()\n",
    "endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "deployment = os.environ[\"CHAT_COMPLETIONS_DEPLOYMENT_NAME\"]\n",
    "search_endpoint = os.environ[\"SEARCH_ENDPOINT\"]\n",
    "search_index = os.environ[\"SEARCH_INDEX\"]\n",
    "      \n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "      \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "      \n",
    "completion = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who is DRI?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"DRI stands for Directly Responsible Individual of a service. Which service are you asking about?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Opinion mining service\"\n",
    "        }\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": search_index,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"system_assigned_managed_identity\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "      \n",
    "print(completion.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Main  Function \n",
    "\n",
    "\n",
    "Let us build a simple `main` function that is an entry point for a program that interacts with the Azure OpenAI service. It takes a function as an argument, which will be called with the Azure OpenAI client and model.\n",
    "\n",
    "H\n",
    "\n",
    "1. **Initialization**: The function starts by loading environment variables using `load_dotenv()`.\n",
    "2. **Azure OpenAI Configuration**: The function retrieves three environment variables:\n",
    "\t* `AZURE_OAI_ENDPOINT`: the endpoint URL for the Azure OpenAI service\n",
    "\t* `AZURE_OAI_KEY`: the API key for the Azure OpenAI service\n",
    "\t* `AZURE_OAI_MODEL`: the model identifier for the Azure OpenAI service\n",
    "3. **Azure OpenAI Client Creation**: The function creates an instance of the `AzureOpenAI` client, passing in the endpoint URL, API key, and API version (`0613`).\n",
    "4. **Function Call**: The function checks if the passed argument `func` is a callable function using the `callable` function. If it is, the function calls the passed function with the Azure OpenAI client and model as arguments.\n",
    "5. **Error Handling**: The function catches any exceptions that may occur during execution and prints the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import utils\n",
    "# Add OpenAI import. (Added code)\n",
    "from openai import AzureOpenAI, Model, ChatCompletion\n",
    "def main(func):\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        utils.initLogFile()\n",
    "        azure_oai_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "        azure_oai_key = os.getenv(\"AZURE_OAI_KEY\")\n",
    "        azure_oai_model = os.getenv(\"AZURE_OAI_MODEL\")\n",
    "        # Define Azure OpenAI client (Add code here)\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=azure_oai_endpoint,\n",
    "            api_key=azure_oai_key,\n",
    "            api_version=\"0613\"\n",
    "             #api_version=\"2023-12-01-preview\"  # Modified to use the specified API version\n",
    "        )\n",
    "        if callable(func):\n",
    "            func(client, azure_oai_model)\n",
    "        else:\n",
    "            print(\"Invalid input. Please pass a valid function.\")\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: Implementing a Proof of Concept (PoC) for Azure OpenAI\n",
    "\n",
    "In this scenario, we're tasked with deploying a GPT-35-turbo-16k model in Azure OpenAI and configuring a sample application to connect to the resources. This serves as our initial PoC to demonstrate the capabilities of Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Validate PoC\n",
    "def function1(aiClient, aiModel):\n",
    "    inputText = utils.getPromptInput(\"Task 1: Validate PoC\", \"sample-text.txt\")\n",
    "    \n",
    "    # Build messages to send to Azure OpenAI model. (Modified code)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": inputText}  # Modified to us\"user\" role and \"content\" key\n",
    "    ]\n",
    "    \n",
    "    # Define argument list (Modified code)\n",
    "    apiParams = {\n",
    "        \"model\": aiModel,  # Added model parameter\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "    utils.writeLog(\"API Parameters:\\n\", apiParams)\n",
    "    # Call chat completion connection. (Modified code)\n",
    "    response = aiClient.chat_completion(**apiParams)  # Modified to use the aiClient and **apiParams\n",
    "    \n",
    "    utils.writeLog(\"Response:\\n\", str(response))\n",
    "    print(\"Response: \" + response.choices[0].message.content + \"\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the main function with function1 as an argument\n",
    "main(function1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Developing the PoC App for Company Chatbot\n",
    "\n",
    "Here, we'll further develop the PoC app to explore the potential of Azure OpenAI for company chatbot functionality. The goal is to develop an app that provides responses in a casual tone, limited to 1,000 tokens, and with a temperature of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Company chatbot\n",
    "def function2(aiClient, aiModel):\n",
    "    inputText = utils.getPromptInput(\"Task 2: Company chatbot\", \"sample-text.txt\")\n",
    "    \n",
    "    # Build messages to send to Azure OpenAI model. (Added code)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": inputText},\n",
    "        {\"role\": \"assistant\", \"content\": \"You can find it on the footer of every page on our website. Hope that helps! Thanks for using Contoso, Ltd.\"}\n",
    "    ]\n",
    "    \n",
    "    # Define argument list (Modified code)\n",
    "    apiParams = {\n",
    "        \"model\": aiModel,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 1000\n",
    "    }\n",
    "    \n",
    "    utils.writeLog(\"API Parameters:\\n\", apiParams)\n",
    "\n",
    "    # Call chat completion connection. (Modified code)\n",
    "    response = aiClient.chat_completion(**apiParams)\n",
    "    utils.writeLog(\"Response:\\n\", str(response))\n",
    "    print(\"Response\"+ response.choices[0].message.content + \"\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Using the PoC App for Developer Tasks\n",
    "\n",
    "In this scenario, we'll use the PoC app to assist with developer tasks such as code refactoring and unit testing. This demonstrates how Azure OpenAI can enhance productivity and streamline development workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Developer tasks\n",
    "def function3(aiClient, aiModel):\n",
    "    inputText = utils.getPromptInput(\"Task 3: Developer tasks\", \"sample-text.txt\")\n",
    "    \n",
    "    # Build messages to send to Azure OpenAI model. (Added code)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": inputText},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}  # Initialize response content\n",
    "    ]\n",
    "    \n",
    "    # Define argument list (Modified code)\n",
    "    apiParams = {\n",
    "        \"model\": aiModel,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.5,  # Set temperature to 0.5\n",
    "        \"max_tokens\": 1000  # Set max tokens to 1000\n",
    "    }\n",
    "    \n",
    "    utils.writeLog(\"API Parameters:\\n\", apiParams)\n",
    "\n",
    "    # Call chat completion connection. (Modified code)\n",
    "    response = aiClient.chat_completion(**apiParams)\n",
    "    \n",
    "    utils.writeLog(\"Response:\\n\", str(response))\n",
    "    print(\"Response: \" + response.choices[0].message.content + \"\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 4: Using Company Data for Travel Recommendations\n",
    "\n",
    "Finally, we'll extend the PoC app to utilize our company's data to better answer customer questions related to travel. The goal is to connect the PoC app to an Azure AI Search resource that contains sample travel data, providing more accurate and relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Use company data\n",
    "def function4(aiClient, aiModel):\n",
    "    inputText = utils.getPromptInput(\"Task 4: Use company data\", \"sample-text.txt\")\n",
    "    \n",
    "    # Build messages to send to Azure OpenAI model. (Added code)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful travel agent.\"},\n",
    "        {\"role\": \"user\", \"content\": inputText}\n",
    "    ]\n",
    "    \n",
    "    # Define connection and argument list (Added code)\n",
    "    from azure.ai.search import SearchClient\n",
    "    from azure.ai.search.models import QueryType\n",
    "    search_client = SearchClient(\"https://search41431948.search.windows.net/\", \"pocindex\")\n",
    "    apiParams = {\n",
    "        \"search_client\": search_client,\n",
    "        \"query\": inputText,\n",
    "        \"query_type\": QueryType.KEYWORD\n",
    "    }\n",
    "    \n",
    "    utils.writeLog(\"API Parameters:\\n\", apiParams)\n",
    "    \n",
    "    # Call search connection. (Added code)\n",
    "    response = search_client.search(**apiParams)\n",
    "    \n",
    "    utils.writeLog(\"Response:\\n\", str(response))\n",
    "    print(\"Response: \" + response.results[0].text + \"\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "azure_oai_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key = os.getenv(\"AZURE_OAI_KEY\")\n",
    "azure_oai_model = os.getenv(\"AZURE_OAI_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "deployment_name = 'tz-edu-gpt-35-turbo' \n",
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "        azure_endpoint = azure_oai_endpoint, \n",
    "        api_key=azure_oai_key,  \n",
    "        api_version=\"2024-02-01\" #  Target version of the API, such as 2024-02-15-preview\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Azure OpenAI resource\n",
    "Once you've configured your connection to Azure OpenAI, send your prompt to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mdeployment_name,\n\u001b[0;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Azure OpenAI?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\azure\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\azure\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    605\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    608\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    609\u001b[0m             {\n\u001b[0;32m    610\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    611\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    612\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    613\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    614\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    615\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    616\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    617\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    618\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    620\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    621\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    622\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    623\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    624\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    625\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    626\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    627\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    628\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    629\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    631\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    632\u001b[0m             },\n\u001b[0;32m    633\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    634\u001b[0m         ),\n\u001b[0;32m    635\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    636\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    637\u001b[0m         ),\n\u001b[0;32m    638\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    639\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    640\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\azure\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\azure\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    927\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\azure\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Azure OpenAI?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = response.choices[0].message.content\n",
    "\n",
    "# Print the response\n",
    "print(\"Response: \" + generated_text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object contains several values, such as total_tokens and finish_reason. The completion from the response object will be similar to the following completion:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
